{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Introduction to Spark using  PySpark\n",
    "\n",
    "###  Configure  Spark\n",
    "\n",
    "1.  Set  environmental variables:\n",
    "    +  *SPARK_HOME*      (Path to spark installation directory)\n",
    "    +  *PYSPARK_PYTHON*  (Path to python executable Ex. /anaconda/bin/python)   \n",
    "      \n",
    "2.  Edit  *spark-env.sh* to add  the following:\n",
    "    +  *SPARK_LOG_DIR*\n",
    "    +  *SPARK_LOCAL_DIRS*\n",
    "    +  *SPARK_WORKER_INSTANCES*\n",
    "    +  *SPARK_WORKER_DIR*\n",
    "    +  *SPARK_CLASSPATH* (Add external libraries such as jars here; adds to both executor & driver classpaths)\n",
    "      +  [Discussion](http://stackoverflow.com/questions/37132559/add-jars-to-a-spark-job-spark-submit)\n",
    "3.  Edit the *logging.properties* (optional)\n",
    "4.  Edit the  *slaves* file to add  any  machines/hosts per line  that will act as worker  nodes\n",
    "\n",
    "###  Start  standalone  spark cluster manager\n",
    "5.  Start the master:     SPARK_HOME/sbin/start-master.sh\n",
    "6.  Access the  WebUI:  [MasterWebUI: Started MasterWebUI at http://xxx.xxx.xxx.xxx:8080]\n",
    "7.  Enable  *Remote Login* on Mac OSX  so that  slaves can  connect to  port 22\n",
    "6.  Start the  slave(s):  SPARK_HOME/sbin/start-slaves.sh  spark://master-host:port\n",
    "\n",
    "###  Stop  master and slaves\n",
    "7.  Stop the  slave(s):  SPARK_HOME/sbin/stop-slaves.sh\n",
    "8.  Stop the  master:    SPARK_HOME/sbin/stop-master.sh\n",
    "\n",
    "####  Note: Add  pyspark modules (SPARK_HOME/python/lib) to  python path  from code  so  that they are available for imports.\n",
    "\n",
    "###  Submitting Spark Applications (Jobs)\n",
    "SPARK_HOME/bin/spark-submit --master //master-url// --py-files //comma-separated python dependencies as zip// our-python-code.py  //custom args//   \n",
    "\n",
    "***HELP***: SPARK_HOME/bin/spark-submit â€”help\n",
    "\n",
    "\n",
    "###  References\n",
    "*  [PySpark API Refenence]( http://spark.apache.org/docs/latest/api/python/index.html )\n",
    "*  [Spark  Programming Guide]( https://spark.apache.org/docs/latest/programming-guide.html )\n",
    "*  [Tutorial on Spark Applications]( https://districtdatalabs.silvrback.com/getting-started-with-spark-in-python )\n",
    "*  [Configure IPython for Spark]( https://gist.github.com/tommycarpi/f5a67c66a8f2170e263c )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.environ[\"SPARK_HOME\"] + \"/python/lib/py4j-0.9-src.zip\")\n",
    "sys.path.append(os.environ[\"SPARK_HOME\"] + \"/python/lib/pyspark.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark import SparkFiles\n",
    "from pyspark import StorageLevel\n",
    "from pyspark import AccumulatorParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sconf = SparkConf()\n",
    "sconf.setAppName(\"PySpark Tutorial\")\n",
    "sconf.setMaster(\"spark://snehasish-barmans-macbook.local:7077\")\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf = sconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x10343d450>\n",
      "1.6.2\n"
     ]
    }
   ],
   "source": [
    "print sc\n",
    "print sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Spark  does  lazy  evaluation.  If we  have a chain of transformations, Spark won't  execute them untill an action is invoked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### A RDD  has  several levels of persistence (Memory, Disk, Memory-DISK, Serialized, ...).  If we use a RDD  for subsequent transformations, it is good to persist it to avoid expensive computations.\n",
    "\n",
    "rdd  \n",
    "rdd.persist()  \n",
    "rdd.is_cached  \n",
    "rdd.trans1().action()  \n",
    "\n",
    "rdd.trans1().trans2().action()  \n",
    "rdd.unpersist()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### No. of partitions = No. of output files for output actions.   \n",
    "###### No. of  partitions = No. of executors. Spark will start & stop an executor to work on each partition of data. Starting & Stopping many executors has an overhead for small size of data with many partitions. \n",
    "######  No. of partitions = Level of parallelism\n",
    "######  Default partitioner = Hash  partitioner ( evenly distribute all the elements of RDD across all the workers )\n",
    "###### 2 - 4 partitions  per CPU core is recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Transformations\n",
    "\n",
    "An RDD can contain heterogenuous elements.  \n",
    "Ex. [1,2, \"abc\", (1,2), {4,5,6}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 'abc', (1, 2), {4, 5, 6}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1,2, \"abc\", (1,2), {4,5,6}]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10], 3)\n",
    "rdd2 = sc.parallelize(xrange(10, 20), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [4, 5, 6], [7, 8, 9, 10]]\n",
      "[[10, 11, 12], [13, 14, 15], [16, 17, 18, 19]]\n"
     ]
    }
   ],
   "source": [
    "print rdd.glom().collect() # shows data grouped by partitions\n",
    "print rdd2.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x: x**2).collect() # map 1-to-1 transformation, operates on every element of rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# functions must be self-contained, no states or access global variables\n",
    "def trans1(x):\n",
    "    return x**2\n",
    "\n",
    "rdd.map(trans1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda x: x > 5).collect() # filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "datasets = \"../../../Machine_Learning/WPI_ML/datasets\"\n",
    "print os.path.exists(datasets)\n",
    "#print os.path.realpath(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# default is hdfs filesystem, to access local files, use namespace -> file:///\n",
    "textrdd = sc.textFile(\"file:///\" + os.path.realpath(datasets) + \"/Audio_Standardization_ Sentences.txt\", \n",
    "                      use_unicode = False, minPartitions = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Oak is strong and also gives shade.',\n",
       "  'Cats and dogs each hate the other.',\n",
       "  'The pipe began to rust while new.',\n",
       "  \"Open the crate but don't break the glass.\"],\n",
       " ['Add the sum to the product of these three.',\n",
       "  'Thieves who rob friends deserve jail.',\n",
       "  'The ripe taste of cheese improves with age.'],\n",
       " ['Act on these orders with great speed.',\n",
       "  'The hog crawled under the high fence.',\n",
       "  'Move the vat over the hot fire.',\n",
       "  'Is this the end.']]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textrdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Oak',\n",
       " 'is',\n",
       " 'strong',\n",
       " 'and',\n",
       " 'also',\n",
       " 'gives',\n",
       " 'shade.',\n",
       " 'Cats',\n",
       " 'and',\n",
       " 'dogs',\n",
       " 'each']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textrdd.flatMap(lambda x: x.split(\" \")).take(11) # 1-to-many transformation, puts in a global list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[29, 23, 25]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def countWordsInPartition(iterator): \n",
    "    \"\"\"\n",
    "    @params:\n",
    "        iterator: a partition of the rdd\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for x in iterator:\n",
    "        count += len(x.split(\" \"))\n",
    "    yield count\n",
    "\n",
    "textrdd.mapPartitions(countWordsInPartition).collect() # same as map but operates on each chunk/partition of the rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 9, 8, 7, 6, 5, 4, 3, 2, 1]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sortBy(keyfunc = lambda x: x, ascending = False, numPartitions = 1).collect() # sorting\n",
    "# numPartitions controls the level of parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 6, 7, 8, 10]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sample(withReplacement = False, fraction = 0.5, seed = 13).collect() # sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.coalesce(1).glom().collect() # reduce no. of partitions by combining partitions from each worker, thereby minimizing network traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5, 6, 7, 8, 9], [10]]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.repartition(2).glom().collect() \n",
    "# increases or decreases the no. of partitions, but at the cost of more network traffic, \n",
    "# because Spark has to shuffle the data across the workers.\n",
    "# Use coalesce when intent to decrease the partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10], [1, 2, 3, 4, 5, 6], [], [], [7, 8, 9]]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.repartition(5).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.union(rdd2).collect() # combines two rdds -> A u B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.intersection(rdd2).collect() # intersection -> A n B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 1, 7, 8, 2, 9, 3, 4, 5]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.subtract(rdd2).collect() # subtract -> A - B, removes all the common elements between A and B from A and returns the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.union(rdd2).distinct().sortBy(ascending = True, keyfunc = lambda x:x).collect() # distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 10), (2, 10), (3, 10), (1, 11), (1, 12)]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.cartesian(rdd2).take(5) # all pair combinations; creates key-value RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 10),\n",
       " (2, 11),\n",
       " (3, 12),\n",
       " (4, 13),\n",
       " (5, 14),\n",
       " (6, 15),\n",
       " (7, 16),\n",
       " (8, 17),\n",
       " (9, 18),\n",
       " (10, 19)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.zip(rdd2).collect() # zip (same as zip() in python); creates key-value RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1),\n",
       " (2, 2),\n",
       " (0, 3),\n",
       " (1, 4),\n",
       " (2, 5),\n",
       " (0, 6),\n",
       " (1, 7),\n",
       " (2, 8),\n",
       " (0, 9),\n",
       " (1, 10)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.keyBy(lambda x: x % 3).collect() # keyBy (converts a normal RDD into key-value RDD based on a criteria)\n",
    "# result of the criteria becomes the 'key' and the element itself becomes the 'value'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, <pyspark.resultiterable.ResultIterable object at 0x105861350>), (1, <pyspark.resultiterable.ResultIterable object at 0x105861f90>), (2, <pyspark.resultiterable.ResultIterable object at 0x105861fd0>)]\n",
      "[3, 6, 9]\n"
     ]
    }
   ],
   "source": [
    "print rdd.groupBy(lambda x: x % 3).collect() # groupBy - same as 'keyBy' but all the values of a key are grouped into an iterable\n",
    "print list(rdd.groupBy(lambda x: x % 3).collect()[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_name = \"square_nums.py\"\n",
    "sc.addFile(\"./\" + file_name) # All workers will download this file to their node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.pipe(\"cat\").collect() # pipe\n",
    "# Use an external program for custom transformations.\n",
    "# Reads data as string per partition from standard input and writes as string to standard output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'1 4 9 '], [u'16 25 36 '], [u'49 64 81 100 ']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.pipe(SparkFiles.get(file_name)).glom().collect() # pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Actions\n",
    "\n",
    "1.  returns anything but rdd.\n",
    "2.  may return nothing as well (foreach).\n",
    "3.  like aggregtions (N to few).\n",
    "4.  return value(s) to driver code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduce(lambda acc, x: acc+x) # reduce; operation must satisfy associative and communtative property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count() # count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(4) # take (returns as a list; selects data from one partition, then moves to another partition as required to satisfy the limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 5, 4, 1, 9]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.takeSample(False, 5, seed = 13) # takeSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.takeOrdered(4, key = lambda x:x) # takeOrdered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.first() # first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 9, 8, 7]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.top(4, key = int) # top ; returns top n items in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.countApprox(1000, 0.5) # countApprox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11L"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.countApproxDistinct(0.7) # number of distinct elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def showValues(x):\n",
    "    print \"hello: \" + str(x)\n",
    "    \n",
    "rdd.foreach(showValues) # foreach (Applies a function to every element of rdd)\n",
    "# useful to communicate to external services, accumulate values in a queue, logging info, ...\n",
    "# NOTE: verify results in stderr file of the working dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def showValuesPartition(iterator):\n",
    "    vals = []\n",
    "    for item in iterator:\n",
    "        vals.append(\"hello: \" + str(item))\n",
    "    print vals\n",
    "        \n",
    "rdd.foreachPartition(showValuesPartition) # foreachPartition (Applies a function per partition of rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count: 10, mean: 5.5, stdev: 2.87228132327, max: 10.0, min: 1.0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8722813232690143"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.stdev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# must be an absolute path to directory name; default is hdfs namespace\n",
    "# creates a part-xxxx file for each partition\n",
    "rdd.saveAsTextFile(\"file:///\" + os.path.realpath(\"./textfiles\")) # saveAsTextFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using compression\n",
    "# compresses part-xxxx file of each partition\n",
    "rdd.saveAsTextFile(\"file:///\" + os.path.realpath(\"./textfileszip\"), \n",
    "                   compressionCodecClass = \"org.apache.hadoop.io.compress.GzipCodec\") # saveAsTextFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd.saveAsPickleFile(\"file:///\" + os.path.realpath(\"./textfiles-pickled\")) # saveAsPickleFile (faster reads, writes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.countByValue() # countByValue - returns as dict of value: count"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rdd.id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.isEmpty() # isEmpty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serialized 1x Replicated\n"
     ]
    }
   ],
   "source": [
    "print rdd.getStorageLevel() # getStorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getNumPartitions() # getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Disk Serialized 1x Replicated\n",
      "False\n",
      "Serialized 1x Replicated\n"
     ]
    }
   ],
   "source": [
    "rdd.persist(StorageLevel.DISK_ONLY)\n",
    "print rdd.is_cached\n",
    "print rdd.getStorageLevel()\n",
    "rdd.unpersist()\n",
    "print rdd.is_cached\n",
    "print rdd.getStorageLevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key-Value RDD Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "krdd = sc.parallelize([(\"a\", 1), (\"a\", 2), (\"b\", 1), (\"b\", 2), (\"c\", 1)], 2)\n",
    "krdd2 = sc.parallelize([(\"a\", 3), (\"b\", 3), (\"d\", 1)], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('a', 1), ('a', 2)], [('b', 1), ('b', 2), ('c', 1)]]\n",
      "[[('a', 3)], [('b', 3), ('d', 1)]]\n"
     ]
    }
   ],
   "source": [
    "print krdd.glom().collect()\n",
    "print krdd2.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', <pyspark.resultiterable.ResultIterable at 0x105a383d0>),\n",
       " ('b', <pyspark.resultiterable.ResultIterable at 0x105a381d0>)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "krdd.groupByKey().collect() # groupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(krdd.groupByKey().collect()[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 3), ('b', 3)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "krdd.reduceByKey(lambda acc, x: acc + x, numPartitions = 1).collect() # reduceByKey\n",
    "# does a groupByKey, followed by reduction\n",
    "# operation must obey associative and commutative properties\n",
    "# numPartitions controls the level of parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', [3]), ('b', [3])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http://www.learnbymarketing.com/618/pyspark-rdd-basics-examples/\n",
    "# does a groupByKey, followed by custom reduce function that doesn't have to obey commutative and associative property\n",
    "\n",
    "# define a resultset template (any data structure) with initial values\n",
    "init_state_template = [0]\n",
    "\n",
    "def mergeValuesWithinPartition(template, val):\n",
    "    template[0] = template[0] + val\n",
    "    return template\n",
    "\n",
    "def mergePartitions(template1, template2):\n",
    "    template = template1[0] + template2[0]\n",
    "    return template\n",
    "    \n",
    "\n",
    "krdd.aggregateByKey(init_state_template, \n",
    "                    mergeValuesWithinPartition, \n",
    "                    mergePartitions).collect() # aggregateByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 1), ('b', 2), ('a', 1), ('a', 2)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "krdd.sortByKey(ascending = False, numPartitions = 1, keyfunc = lambda x: x).collect() # sortByKey (can also use sortBy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 3)), ('a', (2, 3)), ('b', (1, 3)), ('b', (2, 3))]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "krdd.join(krdd2).collect() # join (inner-join in SQL; returns all-pair combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 3)), ('a', (2, 3)), ('c', (1, None)), ('b', (1, 3)), ('b', (2, 3))]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "krdd.leftOuterJoin(krdd2).collect() # leftOuterJoin (left join in SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 3)), ('a', (2, 3)), ('d', (None, 1)), ('b', (1, 3)), ('b', (2, 3))]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "krdd.rightOuterJoin(krdd2).collect() # rightOuterJoin (right join in SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 3)),\n",
       " ('a', (2, 3)),\n",
       " ('d', (None, 1)),\n",
       " ('c', (1, None)),\n",
       " ('b', (1, 3)),\n",
       " ('b', (2, 3))]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "krdd.fullOuterJoin(krdd2).collect() # fullOuterJoin (full join in SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x105a79e10>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x105a53b10>)),\n",
       " ('d',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x105a53b90>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x105a53210>)),\n",
       " ('c',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x105a53690>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x105a53ed0>)),\n",
       " ('b',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x105a53fd0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x105a532d0>))]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "krdd.cogroup(krdd2).collect() # cogroup (returns iterator one for each rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n",
      "[3]\n"
     ]
    }
   ],
   "source": [
    "print list(krdd.cogroup(krdd2).collect()[0][1][0])\n",
    "print list(krdd.cogroup(krdd2).collect()[0][1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print list(krdd.cogroup(krdd2).collect()[2][1][0])\n",
    "print list(krdd.cogroup(krdd2).collect()[2][1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('a', 4), ('b', 1), ('b', 4), ('c', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "krdd.mapValues(lambda x: x**2).collect() # mapValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('a', 4), ('a', 9), ('b', 16), ('b', 25), ('b', 36)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "krdd_val_iter = sc.parallelize([(\"a\", [1,2,3]), (\"b\", [4,5,6])])\n",
    "krdd_val_iter.flatMapValues(lambda x: [y**2 for y in x]).collect() # flatMapValues\n",
    "# works in which value is an iterable object\n",
    "# unpacks all elements in the iterable into their own key-value tuple/pair; puts them in a single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', [1, 4, 9]), ('b', [16, 25, 36])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "krdd_val_iter.mapValues(lambda x: [y**2 for y in x]).collect() # mapValues 1-to-1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'a', 'b', 'b', 'c']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "krdd.keys().collect() # keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 2, 1]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "krdd.values().collect() # values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key-Value RDD Actions\n",
    "\n",
    "All  actions  available for normal RDD are also  available for  Key-Value RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('a', 2), ('b', 1), ('b', 2), ('c', 1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "krdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "krdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('a', 2), ('b', 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "krdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "krdd_dup = sc.parallelize([(\"a\", 1), (\"a\", 1)]) \n",
    "krdd_dup.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'a': 2, 'b': 2, 'c': 1})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "krdd.countByKey() # countByKey - number of times a key appears in the k-v rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "krdd.lookup(\"a\") # lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(2) ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423 []'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "krdd.toDebugString() # toDebugString (identifies recursive dependencies of this rdd for debugging purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 2, 'b': 2, 'c': 1}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "krdd.collectAsMap() # collectAsMap -> return key-value RDD as a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance\n",
    "\n",
    "**Key**: Reduce the number of data shuffles across the cluster.   \n",
    "\n",
    "**Accumulator** :  It is a shared variable that  can store metadata about the data,  gather  statistics  about the  data that can be used for logging, debugging, tracking or other purposes.   \n",
    "1.  It is executed only when an action is invoked on the RDD.\n",
    "2.  It  doesn't reset to initial value automatically after a transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n",
      "No. of elements: 10\n"
     ]
    }
   ],
   "source": [
    "# default accumulator accumulates only numeric (int and float) types; only does 'add' operation (commutative and associative)\n",
    "accum = sc.accumulator(0, accum_param = None)\n",
    "\n",
    "def squareValues(x):\n",
    "    global accum\n",
    "    #accum += 1\n",
    "    accum.add(1)\n",
    "    return x**2\n",
    "    \n",
    "print rdd.map(squareValues).collect()\n",
    "print \"No. of elements: %d\" % accum.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom accumulator to support any types\n",
    "class CustomAccumulator(AccumulatorParam):\n",
    "    \n",
    "    def zero(self, initialValue):\n",
    "        template = set()\n",
    "        template.add(initialValue)\n",
    "        return template\n",
    "    \n",
    "    def addInPlace(self, template1, template2):\n",
    "        return template1.union(template2)\n",
    "    \n",
    "accum = sc.accumulator(None, accum_param = CustomAccumulator())\n",
    "\n",
    "def squareValues(x):\n",
    "    global accum\n",
    "    accum += x\n",
    "    return x**2\n",
    "    \n",
    "print rdd.map(squareValues).collect()\n",
    "print \"No. of elements: %d\" % accum.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Broadcast variable**:  Another  shared variable  useful to disseminate  small amounts of data to every  executor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 10, 'b': 15}\n"
     ]
    }
   ],
   "source": [
    "bb = sc.broadcast({\"a\": 10, \"b\": 15})\n",
    "print bb.value\n",
    "bb.unpersist() # deletes cached copies from the executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 10, 'b': 15}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
